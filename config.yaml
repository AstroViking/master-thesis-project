
experiments:
  - model: 
      type: "FeedForwardNet"
      parameters:
        hidden_layer_width: 100
        num_hidden_layers: 100
        non_linearity: "tanh"
    training:
      initial:
        dataset: "EMNIST-letters"
        num_epochs: 20
        batch_size: 400
        learning_rate: 0.001
      transfer:
        dataset: "MNIST"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
    evaluation:
      num_samples_per_class: 10
    
  - model: 
      type: "FeedForwardNet"
      parameters:
        hidden_layer_width: 100
        num_hidden_layers: 100
        non_linearity: "tanh"
    training:
      initial:
        dataset: "MNIST"
        num_epochs: 20
        batch_size: 100
        learning_rate: 0.001
      transfer:
        dataset: "FashionMNISTT"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
    evaluation:
      num_samples_per_class: 10

  - model: 
      type: "FeedForwardNet"
      parameters:
        hidden_layer_width: 100
        num_hidden_layers: 20
        non_linearity: "tanh"
    training:
      initial:
        dataset: "FashionMNIST"
        num_epochs: 20
        batch_size: 100
        learning_rate: 0.001
      transfer:
        dataset: "MNIST"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
    evaluation:
      num_samples_per_class: 10

  - model: 
      type: "ConvolutionalNet"
      parameters:
        hidden_layer_width: 32
        num_hidden_layers: 20
        non_linearity: "selu"
    training:
      initial:
        dataset: "MNIST"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
      transfer:
        dataset: "EMNIST-letters"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
    evaluation:
      num_samples_per_class: 10

  
  - model: 
      type: "ConvolutionalNet"
      parameters:
        hidden_layer_width: 32
        num_hidden_layers: 20
        non_linearity: "selu"
    training:
      initial:
        dataset: "CIFAR10"
        num_epochs: 200
        batch_size: 100
        learning_rate: 0.001
      transfer:
        dataset: "MNIST"
        num_epochs: 5
        batch_size: 100
        learning_rate: 0.001
    evaluation:
      num_samples_per_class: 10